/** \page blade Running Jobs at the Blade-Center
The MPI-P has a 28 node Blade-Cluster at the RZG in Garching (see http://www.mpip-mainz.mpg.de/theory/computers/blade/index.html and 
http://www.rzg.mpg.de/docs/linux/bladecenter.html) which may be used with Espresso as well. However, thanks to a certain custom level
in their installation, some slight adjustments are required:<BR>

<b>Only once</b> you'll have to do the following to setup a suitable environment there:
<ul><li>Transfer your Espresso-directory to <tt>ibmr.rzg.mpg.de</tt> along with the following files:
    <ul><li>/usr/local/lib/libfftw.a
        <li>/usr/local/lib/libfftw.la
	<li>/usr/local/lib/librfftw.a
	<li>/usr/local/lib/librfftw.la
	<li>/usr/include/fftw.h
    </ul>
    <li>Login to <tt>ibmr.rzg.mpg.de</tt> and edit <tt>Makefile.Linux</tt> by setting \verbatim BLADE=yes\endverbatim
    <li>Prepare (another) script-file <tt>submit_parallel.sh</tt> for the queuing system (see http://www.rzg.mpg.de/docs/linux/sge.html, 
        but note that their example script does <i>not</i> work) which should look like:<BR>
\verbatim
#!/bin/sh
###################################################################
# SGE options:
#
# Change to the current working directory upon starting of the job
#$ -cwd
#
# join the error and standard output streams
#$ -j y
#
# use the parallel environment mpich with 1 (or 2) processor(s)
#$ -pe mpich 1
#
# set the required resources (up) to 96 hours of computing
#$ -l h_rt=96:00:00
# do _not_ attempt to request num_proc here, or your job may not run, since num_proc is only a local resource.
#
# don't flood myself with e-mail
#$ -m n
#
# notify me about pending SIG_STOP and SIG_KILL
#$ -notify
#
# name of the job
#$ -N diamond_NpT
#
# end of SGE stuff
###################################################################
# now execute my job:

export ESPRESSO_SOURCE=/afs/ipp-garching.mpg.de/home/b/bhm/Espresso
export ESPRESSO_SCRIPTS=/afs/ipp-garching.mpg.de/home/b/bhm/Espresso/scripts
/afs/ipp-garching.mpg.de/i386_linux24/bin/mpirun -np 1 $ESPRESSO_SOURCE/Linux/Espresso diamond_NpT.tcl

# end of job script
###################################################################
\endverbatim
     You'll have to replace '<tt>/home/b/bhm/Espresso</tt>' with the appropriate (absolute!) path to your home-directory (if that's where your
     Espresso-copy is residing), but note that path-abbreviations such as <tt>~/Espresso</tt> will <i>not</i> work over there!<BR>
     In addition, replace '<tt>diamond_NpT.tcl</tt>' with the tcl-script you want to run, and change '<tt>-np 1</tt>' if you wanted more
     than one processor.
</ul>

<b>Each time</b> you want to use the Blade-Center have at look at these things:
<ul><li>Transfer your tcl-script to <tt>ibmr.rzg.mpg.de</tt> and login there.
    <li>Remove the header of your tcl-script (the stuff like "<tt>tricking...</tt>" 'til "<tt>fi;</tt>") 
        and copy it to your desired output-location, preferable the local scratch areas <tt>/gmpip/mpo/$USER</tt>.
    <li>Copy that <tt>submit_parallel.sh</tt>-script from above to wherever your tcl-script just went, and go there, too.
    <li>Edit <tt>submit_parallel.sh</tt> to have the correct tcl-script-name and the desired CPU-numbers and -time.
    <li>Use \verbatim qsub submit_parallel.sh\endverbatim to launch your simulation, and \verbatim qstat -f\endverbatim to check all nodes' status.
    <li>The joined <tt>stdout</tt> and <tt>stderr</tt> are written to <tt>\<Name\>.o\<PID\></tt> in your current directory, 
        where <tt>\<Name\></tt> is whatever you specified for the <tt>-N</tt>-option in the job-script and <tt>\<PID\></tt> is your job's process number.
</ul>
*/
