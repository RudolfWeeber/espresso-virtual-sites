/** \page tcl_cellsystem Cell systems
This page deals with the flexible particle data organization of Espresso. Due to different needs of different
algorithms, Espresso is able to change the organization of the particles in the computer memory, according to
the needs of the used algorithms. The cellsystem is change by the command
\verbatim cellsystem <system> <parameters> \endverbatim

For details on the possible cellsystems and their parameters see the sections below.
For details on the internal organization see \ref cellsystem_details. The C implementation of
the cellsystem command is \ref #cellsystem.

\section cellsystem_domdec Domain decomposition
\verbatim cellsystem domain_decomposition ?-?no?_verlet_list? \endverbatim
This selects the domain decomposition cell scheme, using Verlet for the calculation of the
interactions. If you specify -no_verlet_list, only the domain decomposition is done, but no Verlet
lists.

The domain decomposition cellsystem is the default system and suits most applications with short
ranged interactions. The particles are divided up spatially into small compartments, the cells,
such that the cell size is larger than the maximal interaction range. In this case interactions only
occur between particles in adjacent cells. Since the interaction range should be  much smaller than
the total system size, leaving out all interactions between non-adjacent cells can mean a tremendous
speed-up. Moreover, since for constant interaction range, the number of particles in a cell depends only
on the density. The number of interactions is therefore of the order N instead of order N^2 if one has
to calculate all pair interactions. For further details, see \ref domain_decomposition.h "domain_decomposition.h".

\section cellsystem_nsq N-squared
\verbatim nsquare \endverbatim
This selects the very primitive nsquared cellsystem, which calculates the interactions for all particle
pairs. Therefore it loops overall particles, giving an unfavorable computation time scaling of N^2. However,
algorithms like MMM1D or the plain Coulomb interaction in the cell model require the calculation of all pair
interactions.

In a multiple processor environment, the nsquared cellsystem uses a simple particle balancing scheme to have a nearly
equal number of particles per CPU, i. e. n nodes have m particles, and p-n nodes have m+1 particles, such that
n*m+(p-n)*(m+1)=N, the total number of particles. Therefore the computational load should be balanced fairly equal among
the nodes, with one exception: This code always uses one CPU for the interaction between two different nodes. For an
odd number of nodes, this is fine, because the total number of interactions to calculate is a multiple of the number of
nodes, but for an even number of nodes, for each of the p-1 communication rounds, one processor is idle.

E. g. for 2 processors, there are 3 interactions: 0-0, 1-1, 0-1. Naturally, 0-0 and 1-1 are treated by processor 0 and 1,
respectively. But the 0-1 interaction is treated by node 1 alone, so the workload for this node is twice as high. For 3
processors, the interactions are 0-0, 1-1, 2-2, 0-1, 1-2, 0-2. Of these interactions, node 0 treats 0-0 and 0-2, node 1
treats 1-1 and 0-1, and node 2 treats 2-2 and 1-2.

Therefore it is highly recommended that you use nsquared only with an odd number of nodes, if with multiple processors
at all. For further details, see nsquare.h.

\section cellsystem_layered Layered cell system
\verbatim layered ?<n_layers>? \endverbatim
This selects the layered cell system, which is specifically designed for the needs of the MMM2D algorithm. Basically
it consists of a nsquared algorithm in x and y, but a domain decomposition along z, i. e. the system is cut into equally
sized layers along the z axis. The current implementation allows for the cpus to align only along the z axis, therefore
the processor grid has to have the form 1x1xN. However, each processor may be responsible for several layers, which is
determined by \<n_layers\>, i. e. the system is split into N*\<n_layers\> layers along the z axis. Since in x and y direction
there are no processor boundaries, the implementation is basically just a stripped down version of the domain decomposition
cellsystem.

\section cellsystem_details Internal particle organization

Since basically all major parts of the main MD integration have to access the particle data, efficient access to the particle data
is crucial for a fast MD code. Therefore the particle data needs some more elaborate organisation, which will be presented here. A
particle itself is represented by a structure (\ref Particle) consisting of several substructures (e. g. \ref ParticlePosition,
\ref ParticleForce or \ref ParticleProperties), which in turn represent basic physical properties such as position, force or charge.
The particles are organised in one or more \ref ParticleList "particle lists" on each node, called \ref Cell cells. The cells are
arranged by several possible systems, the cellsystems as described above.  A cell system defines a way the particles are stored in
Espresso, i. e. how they are distributed onto the processor nodes and how they are organised on each of them. Moreover a cell system
also defines procedures to efficiently calculate the force, energy and pressure for the short ranged interactions, since these can
be heavily optimised depending on the cell system. For example, the domain decomposition cellsystem allows an order N interactions
evaluation.

<img src="../figs/datastorage.gif">

Technically, a cell is organised as a dynamically growing array, not as a list. This ensures that the data of all particles in a
cell is stored contiguously in the memory. The particle data is accessed transparently through a set of methods common to all cell
systems, which allocate the cells, add new particles, retrieve particle information and are responsible for communicating the
particle data between the nodes. Therefore most portions of the code can access the particle data safely without direct knowledge
of the currently used cell system. Only the force, energy and pressure loops are implemented separately for each cell model as
explained above.

The domain decomposition or link cell algorithm is implemented in Espresso such that the cells equal the Espresso cells,
i. e. each cell is a separate particle list. For an example let us assume that the simulation box has size \f$20\times 20\times
20\f$ and that we assign 2 processors to the simulation. Then each processor is responsible for the particles inside a \f$10\times
20\times 20\f$ box. If the maximal interaction range is 1.2, the minimal possible cell size is 1.25 for 8 cells along the first
coordinate, allowing for a small skin of 0.05. If one chooses only 6 boxes in the first coordinate, the skin depth increases to
0.467. In this example we assume that the number of cells in the first coordinate was chosen to be 6 and that the cells are
cubic. Espresso would then organise the cells on each node in a \f$6\times 12\times 12\f$ cell grid embedded at the centre of a
\f$8\times 14 \times 14\f$ grid. The additional cells around the cells containing the particles represent the ghost shell in which
the information of the ghost particles from the neighbouring nodes is stored. Therefore the particle information stored on each
node resides in 1568 particle lists of which 864 cells contain particles assigned to the node, the rest contain information of
particles from other nodes.a

Classically, the link cell algorithm is implemented differently. Instead of having separate particle lists for each cell, there is
only one particle list per node, and a the cells actually only contain pointers into this particle list. This has the advantage that
when particles are moved from one cell to another on the same processor, only the pointers have to be updated, which is much less data
(4 rsp. 8 bytes) than the full particle structure (around 192 bytes, depending on the features compiled in). The data storage scheme
of Espresso however requires to always move the full particle data. Nevertheless, from our experience, the second approach is 2-3 times
faster than the classical one.

To understand this, one has to know a little bit about the architecture of modern computers. Most modern processors have a clock
frequency above 1GHz and are able to execute nearly one instruction per clock tick. In contrast to this, the memory runs at a
clock speed around 200MHz. Modern double data rate (DDR) RAM transfers up to 3.2GB/s at this clock speed (at each edge of the
clock signal 8 bytes are transferred). But in addition to the data transfer speed, DDR RAM has some latency for fetching the data,
which can be up to 50ns in the worst case. Memory is organised internally in pages or rows of typically 8KB size. The full
\f$2\times 200\f$ MHz data rate can only be achieved if the access is within the same memory page (page hit), otherwise some latency
has to be added (page miss). The actual latency depends on some other aspects of the memory organisation which will not be
discussed here, but the penalty is at least 10ns, resulting in an effective memory transfer rate of only 800MB/s. To remedy this,
modern processors have a small amount of low latency memory directly attached to the processor, the <em>cache</em>.

The processor cache is organised in different levels. The level 1 (L1) cache is built directly into the processor core, has no
latency and delivers the data immediately on demand, but has only a small size of around 128KB. This is important since modern
processors can issue several simple operations such as additions simultaneously. The L2 cache is larger, typically around 1MB, but
is located outside the processor core and delivers data at the processor clock rate or some fraction of it.

In a typical implementation of the link cell scheme the order of the particles is fairly random, determined e. g. by the order in
which the particles are set up or have been communicated across the processor boundaries. The force loop therefore accesses the
particle array in arbitrary order, resulting in a lot of unfavourable page misses. In the memory organisation of Espresso, the
particles are accessed in a virtually linear order. Because the force calculation goes through the cells in a linear fashion, all
accesses to a single cell occur close in time, for the force calculation of the cell itself as well as for its neighbours. Using
the domain decomposition cell scheme, two cell layers have to be kept in the processor cache. For 10000 particles and a typical
cell grid size of 20, these two cell layers consume roughly 200 KBytes, which nearly fits into the L2 cache. Therefore every cell has
to be read from the main memory only once per force calculation.

*/
